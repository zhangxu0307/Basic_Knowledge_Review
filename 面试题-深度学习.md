

# 深度学习

## batch normalization

- https://zhuanlan.zhihu.com/p/34480619
- https://www.cnblogs.com/hansjorn/p/6298576.html

- 算法：
![image](https://pic3.zhimg.com/80/v2-b88a04d2cd8a786403a5b1d1b7202675_hd.jpg)
注意：
1. 均值和方差都是在wx+b计算得到的z上求得的
2. 不能在batch小的情况下使用
3. BN使用的一般位置是在wx+b之后，激活函数之前
4. 测试阶段记录之前数据集中全部batch的均值方差，使用均值和方差的期望作为测试时的均值和方差,其中var是无偏估计
5. 当 `$(\mu,\delta)$`与`$ (\gamma,\beta)$` 对应相等的时候，那么BN就相当于没有做
当 `$(\mu,\delta)$`与`$ (\gamma,\beta)$` 不等的时候，就是BN起作用的时候了

![image](http://img.blog.csdn.net/20150313164032752)


- 优点
1. 加速收敛
2. 缓解梯度消失梯度爆炸
3. 可以使用更大的学习率
4. 避免overfit，可以降低正则项权重和丢弃dropout
5. 


## Resnet

https://blog.csdn.net/lanran2/article/details/79057994
https://blog.csdn.net/shwan_ma/article/details/78203020

- 解决的问题：不是梯度消失，而是网络退化（degradation）

网咯加深，效果应该不差于浅层网络，实际并不是
说明网络拟合恒等映射也很困难。
如下图，可以看出拟合输入输出之间残差，拟合难度就会降低。

![image](https://img-blog.csdn.net/20170220201128938?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3NwYmE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

- 两种结构：左边是basic结构，resnet18和34采用，后面resnte50,101,152使用右边的bottlecnk结构，减少参数和运算量。

![image](https://img-blog.csdn.net/20180114183212429?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGFucmFuMg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

- 网络结构

![image](https://img-blog.csdn.net/20180114205444652?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGFucmFuMg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


- 注意：
1. resnet第一层使用7\*7大卷积核卷积，后面则使用3\*3卷积核
2. 无全连接层，直接7\*7平均池化
3. block层间无pooling结构，每个block的首层使用stride=2实现降采样，四个block依次减半。变化为224->112(conv1)->56(max pool)->28(block2)->14(block3)->7(block4)->1(avg_pool)
4. 图像通道变化3->64->64->128->256->512

- 变体：
1. preact-resnet：在激活之后加残差，使得block真正拟合的是残差。

![image](http://img.mp.itc.cn/upload/20170722/9a4876173be440ba84fc9f3fc77ff669_th.jpg)

2. resnetXt: 加入googlenet的思路，网络变宽了

![image](https://img-blog.csdn.net/20171012220131958?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2h3YW5fbWE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

3. denseNet： 之前的若干层也连接起来，不加和，而是使用concat

![image](https://img-blog.csdn.net/20171006222941946?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2h3YW5fbWE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## 深度学习常见防止过拟合方法

- Early stop：在模型训练过程中，提前终止
- Data expending：用更多的数据集，或者对原始数据做数据扩张，进行图像随机变换
- 正则：L1 L2正则，pytorch中叫weight decay，一般去e-4数量级
- Droup Out：以一定的概率使某些神经元停止工作，可以从ensemble的角度来看，一般取0.5
- BatchNorm


## 深度学习常用激活函数

- sigmoid

1. `$f(x)=1/1+exp(-x)$`，求导`$f'(x) = f(x)(1-f(x))$` 
2. 取值0-1之间，有梯度饱和区
3. 不是0均值的
4. 目的：降输出压缩到0-1之间== ==

- tanh

1. `$f(x)=exp(x)-exp(-x)/exp(x)+exp(-x)$`
2. -1-1之间，梯度饱和区
3. 是0均值的


- relu 

1. `$f(x)=max(0, x)$`
2. 无饱和区 不易产生梯度消失
3. 训练小于0后会dead

- Leaky ReLU

1. `$f(x)=max(x, alpha * x)$`
2. alpha取值在0.01左右
3. 防止dead发生

- prelu

1. `$f(x)=max(x, alpha * x)$`
2. alpha是一个参数加入训练

- Maxout==：==

https://blog.csdn.net/garfielder007/article/details/50581021

![image](https://images0.cnblogs.com/blog/381513/201311/18100510-a14ba98a40a543c099f00d1aa104d2ff.png)
![image](https://images0.cnblogs.com/blog/381513/201311/18100518-7c567fbd32f64745979203a4785e1ab3.png)

参数增加了k倍，因为是在k组参数中选出最大的一组


## 卷积的实现

https://www.jianshu.com/p/244c7340984e

im2col
![image](https://upload-images.jianshu.io/upload_images/2917770-5f05993f96fbfe25.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700)

矩阵乘法
![image](https://upload-images.jianshu.io/upload_images/2917770-e27316d757d53b77.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700)

col2img

![image](https://upload-images.jianshu.io/upload_images/2917770-e5ffa2aa705e02e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700)


多个卷积核只需要把卷积核按行排列，矩阵乘法就可以得到所有的feature map

## 常见损失函数，包括人脸识别的特殊loss

- 交叉熵
    
    用于分类问题

    `$z_i = \sum_j{w_{ij} x_{ij} + b}$`
    
    `$a_i = \frac{e^{z_i}}{\sum_k{e^{z_k}}}$`
    
    `$C = -\sum_i{y_i \ln {a_i}}$`
    
    与softmax的关系
    
    当cross entropy的输入P是softmax的输出时，cross entropy等于softmax loss
    https://blog.csdn.net/u014380165/article/details/77284921
    
    优点：
    
     加大对预测错误的惩罚，相对于直接normalize
    
     缓解梯度消失现象
   
     加快收敛，错误越大，loss下降越快
     
     ![image](https://img-blog.csdn.net/20160402175137034)
     ![image](https://img-blog.csdn.net/20160402180457695)
     https://blog.csdn.net/u014313009/article/details/51043064
    

- 平方误差



- KL散度

    ![image](https://img-blog.csdn.net/20151007094108222?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
    
    不对称性
    非负性
    
    
[KL散度性质](https://blog.csdn.net/acdreamers/article/details/44657745)
    
[信息熵为何如此定义](https://zhuanlan.zhihu.com/p/)
    

- center loss
- constrc loss
- L-softmax
- angloe loss
- AM loss
 

### 卷积参数量计算

input是H\*W\*C, 卷积输出是H\*W\*D, 卷积核尺寸大小K\*K,卷积参数为


```math
H*W*C*D*K*K
```

降低卷积参数量的操作

1\*1卷积

depth-wise卷积



## 网络压缩模型

- squeeze net
- mobile net
- mobile net v2
- shuffle net


## RNN、LSTM及GRU正向传播公式

1. RNN

![image](https://upload-images.jianshu.io/upload_images/2332367-b30d467b9ce27703.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700)

2. LSTM
![image](http://upload-images.jianshu.io/upload_images/42741-96b387f711d1d12c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image](http://upload-images.jianshu.io/upload_images/42741-7fa07e640593f930.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![image](http://upload-images.jianshu.io/upload_images/42741-d88caa3c4faf5353.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
变体1 peehole
![image](http://upload-images.jianshu.io/upload_images/42741-0f80ad5540ea27f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
变体2 
![image](http://upload-images.jianshu.io/upload_images/42741-bd2f1feaea22630e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


3. GRU
![image](https://upload-images.jianshu.io/upload_images/42741-dd3d241fa44a71c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700)


## RNN梯度消失原因

- 简单解释

https://zhuanlan.zhihu.com/p/28871960

前馈神经网络（包括全连接层、卷积层等）可以表示为 `$ F=f_3(f_2(f_1(\mathbf{x}W_1)W_2)W_3) $`，那么网络输出对 `$W_1 $`求偏导  `$\frac{\partial{F}}{\partial{W_1}}=\mathbf{x}*f'_1*W_2*f'_2*W_3*f'_3 $`，这里 `$W_1,W_2,W_3$`是相互独立的，一般不会有数值问题，主要问题在于激活函数的导数 f'在饱和区接近于零，导致梯度消失。

循环神经网络的状态循环部分可以表示为 `$\mathbf{h}_3=f_3(f_2(f_1(\mathbf{h}_0W)W)W)$`，这里的问题不仅在于激活函数的导数，还有 W 在不同时刻是共享的，网络输出对 W 的偏导包含  W 的连乘项，稍有不慎（ W值偏小或偏大）就会出现梯度消失或爆炸。

- 复杂解释加数学推导

https://zhuanlan.zhihu.com/p/33594517

## RNN BPTT算法反向传播公式推导

![image](https://pic1.zhimg.com/80/v2-3bc30e9a2d740fff417e47dcf7d636bc_hd.jpg)

https://www.cnblogs.com/rongyux/p/6715235.html

https://www.cnblogs.com/pinard/p/6509630.html

https://zhuanlan.zhihu.com/p/26892413

BPTT主要更新的参数有三个，跟别是U,W,V

其中V是输出权重，比较好求，类似于标准BP

![image](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+o_%7Bt%7D%5E%7B%2A%7D%7D+%3D+%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+o_%7Bt%7D%7D%2A%5Cfrac%7B%5Cpartial+o_%7Bt%7D%7D%7B%5Cpartial+o_%7Bt%7D%5E%7B%2A%7D%7D+%3D+%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+o_%7Bt%7D%7D%2A%5Cvarphi%5E%7B%27%7D%5Cleft%28+o_%7Bt%7D%5E%7B%2A%7D+%5Cright%29)

![image](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+V%7D+%3D+%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+Vs_%7Bt%7D%7D+%5Ctimes+%5Cfrac%7B%5Cpartial+Vs_%7Bt%7D%7D%7B%5Cpartial+V%7D+%3D+%5Cleft%28+%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+o_%7Bt%7D%7D%2A%5Cvarphi%5E%7B%27%7D%5Cleft%28+o_%7Bt%7D%5E%7B%2A%7D+%5Cright%29+%5Cright%29+%5Ctimes+s_%7Bt%7D%5E%7BT%7D)

难点在于U和W的更新

1. 方向1，沿时间方向反向传播，与w有关
2. 方向2，沿输入传播，与U有关，类似于普通bp
3. 各个时间步的梯度求和，更新权重



目标是求这两个东西

其中第二项是好求的，就看第一项怎么求。

![image](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+U%7D+%26%3D+%5Csum_%7Bk+%3D+1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+s_%7Bk%7D%5E%7B%2A%7D%7D+%5Ctimes+%5Cfrac%7B%5Cpartial+s_%7Bk%7D%5E%7B%2A%7D%7D%7B%5Cpartial+U%7D%7D+%3D+%5Csum_%7Bk+%3D+1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+s_%7Bk%7D%5E%7B%2A%7D%7D+%5Ctimes+x_%7Bk%7D%5E%7BT%7D%7D+%5C%5C+%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+W%7D+%26%3D+%5Csum_%7Bk+%3D+1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+s_%7Bk%7D%5E%7B%2A%7D%7D+%5Ctimes+%5Cfrac%7B%5Cpartial+s_%7Bk%7D%5E%7B%2A%7D%7D%7B%5Cpartial+W%7D%7D+%3D+%5Csum_%7Bk+%3D+1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+s_%7Bk%7D%5E%7B%2A%7D%7D+%5Ctimes+s_%7Bk+-+1%7D%5E%7BT%7D%7D+%5Cend%7Balign%7D+)

具体展开求解参考第一个链接的内容

![image](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+s_%7Bt%7D%5E%7B%2A%7D%7D+%26%3D+%5Cfrac%7B%5Cpartial+s_%7Bt%7D%7D%7B%5Cpartial+s_%7Bt%7D%5E%7B%2A%7D%7D%2A%5Cleft%28+%5Cfrac%7B%5Cpartial+s_%7Bt%7D%5E%7BT%7DV%5E%7BT%7D%7D%7B%5Cpartial+s_%7Bt%7D%7D+%5Ctimes+%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+Vs_%7Bt%7D%7D+%5Cright%29+%3D+%5Cphi%27%28s_t%5E%2A%29%2A%5Cleft%5BV%5E%7BT%7D+%5Ctimes+%5Cleft%28+%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+o_%7Bt%7D%7D%2A%5Cvarphi%5E%7B%27%7D%5Cleft%28+o_%7Bt%7D%5E%7B%2A%7D+%5Cright%29+%5Cright%29%5Cright%5D+%5C%5C+%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+s_%7Bk+-+1%7D%5E%7B%2A%7D%7D+%26%3D+%5Cfrac%7B%5Cpartial+s_%7Bk%7D%5E%7B%2A%7D%7D%7B%5Cpartial+s_%7Bk+-+1%7D%5E%7B%2A%7D%7D+%5Ctimes+%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+s_%7Bk%7D%5E%7B%2A%7D%7D+%3D+%5Cphi%5E%7B%27%7D%5Cleft%28+s_%7Bk+-+1%7D%5E%7B%2A%7D+%5Cright%29+%2A+%5Cleft%28+W%5E%7BT%7D+%5Ctimes+%5Cfrac%7B%5Cpartial+L_%7Bt%7D%7D%7B%5Cpartial+s_%7Bk%7D%5E%7B%2A%7D%7D+%5Cright%29%2C%5C+%5C+%28k+%3D+1%2C%5Cldots%2Ct%29+%5Cend%7Balign%7D)


## seq2seq attention机制

- seq2seq

![image](https://img-blog.csdn.net/20160129150702992?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

- 四种解码模型，包括attention机制

http://jacoxu.com/encoder_decoder/

![image](http://ww3.sinaimg.cn/mw690/697b070fjw1f27r247418j20e50cfdhc.jpg)
![image](http://ww1.sinaimg.cn/mw690/697b070fjw1f27r24o2ctj20ea0co0u8.jpg)
![image](http://ww3.sinaimg.cn/mw690/697b070fjw1f27r2531y2j20f40d20ub.jpg)
![image](http://ww2.sinaimg.cn/mw690/697b070fjw1f27r25j290j20ef0d0gn2.jpg)

- attention机制

![image](https://img-blog.csdn.net/20170722185534137?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3V6cUNob20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

我的博客解析了pytorch的 toy 代码

https://zhuanlan.zhihu.com/p/35955689



## GAN模型基础


## 优化算法基础

- sgd
- bgd
- mini batch sgd
- adam
- RMSrop
- adadelta

## 常见图像扩张方法

- 旋转
- 水平垂直翻转
- jittering
- 随机crop
- 中心crop
- 缩放
- 加噪声


## 常见深度学习物体检测算法

- RCNN
- fast RCNN
- faster RCNN
- SPP net
- YOLO
- SSD


## 卷积padding方式


## 各种经典CNN模型概览

- vgg
- google net
- inception v1-v4
- resnet
- ZF net
- SE net


## 深度学习适用场景分析